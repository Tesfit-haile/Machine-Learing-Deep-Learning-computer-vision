{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6532c21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f568b306",
   "metadata": {},
   "source": [
    "# Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f137a5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "485c47f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set variables\n",
    "lr = 0.001\n",
    "input_size = 784\n",
    "hidden_layer = 100\n",
    "num_classes = 10\n",
    "num_epoch = 2\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddf6f8b",
   "metadata": {},
   "source": [
    "# Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df0bf146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./dataMNIST/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6918880564c748f58f3bc82198ba0efd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./dataMNIST/MNIST/raw/train-images-idx3-ubyte.gz to ./dataMNIST/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./dataMNIST/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e79e3de57484208bf167195d3fba5db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./dataMNIST/MNIST/raw/train-labels-idx1-ubyte.gz to ./dataMNIST/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./dataMNIST/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0518e6237efe474ba06cac54ab5a455e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./dataMNIST/MNIST/raw/t10k-images-idx3-ubyte.gz to ./dataMNIST/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./dataMNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb625a109bb54e42aaacd94a022a663d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./dataMNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./dataMNIST/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = torchvision.datasets.MNIST(root='./dataMNIST', train=True, transform=transforms.ToTensor(),download=True)\n",
    "test_data = torchvision.datasets.MNIST(root='./dataMNIST', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8449ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 1, 28, 28]), '==========', torch.Size([100]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "images, labels = batch\n",
    "images.shape, '==========', labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59f93950",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### (torch.Size([100, 1, 28, 28])\n",
    "# 100 ---> size of batch\n",
    "# 1 -----> num of channel\n",
    "# 28*28 --> size of image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad67b54",
   "metadata": {},
   "source": [
    "# We can see some images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4419b0fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(images[0][0]) # this is the first image splited in to 28 array or tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b13b0ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f90d00cac40>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# so get the index of the 100 images then plot them\n",
    "plt.imshow(images[1][0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f99187",
   "metadata": {},
   "source": [
    "# Set up fully connecte nn with one hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54e776fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_layer, num_classes):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        #self.input_size = input_size\n",
    "        self.lin_1 = nn.Linear(input_size, hidden_layer)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.lin_2 = nn.Linear(hidden_layer, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.lin_1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.lin_2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = NeuralNetwork(input_size, hidden_layer, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b8a9c6",
   "metadata": {},
   "source": [
    "# Loss & optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3be351e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c798f9",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5503757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.3230\n",
      "Loss: 0.4158\n",
      "Loss: 0.2451\n",
      "Loss: 0.3797\n",
      "Loss: 0.2706\n",
      "Loss: 0.2671\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "    # the loop below, train_loader --> will run 6 times (every iter will have 100 images)\n",
    "    for ix, (images, labels) in enumerate(train_loader): \n",
    "        # (images, labels) 100 img and labels in the form of [100, 1, 28, 28]\n",
    "        # BUT we need to reshape it in to [100, 784] but why ??? \n",
    "        images = images.reshape(-1, 28*28).to(device) # torch.Size([100, 784])\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        pred_outputs = model(images)\n",
    "        \n",
    "        loss = loss_func(pred_outputs, labels)        \n",
    "        #backwards\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (ix) % 100 == 0:\n",
    "            print (f'Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c32423",
   "metadata": {},
   "source": [
    "# Testing -------- we need to take care of the grad\n",
    "### During the testing process we should NOT let our model mimorize the data by calculating the gradient descent !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03a80332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "93.52\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correc_pred = 0\n",
    "    n_samples = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outpu_predImg = model(images)\n",
    "        _, predictions = torch.max(outpu_predImg, 1) # will help us to find the maximum num\n",
    "        good_pred = (predictions == labels).sum().item() #first we'll get the pred then will add them then get the num\n",
    "        \n",
    "        n_samples += labels.shape[0]\n",
    "        correc_pred += good_pred\n",
    "        print(n_samples)\n",
    "    \n",
    "    print(100.0 * correc_pred / n_samples)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b1f92b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

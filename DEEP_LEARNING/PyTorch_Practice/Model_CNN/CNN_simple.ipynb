{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e89ed998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "torch.set_printoptions(linewidth=140)\n",
    "\n",
    "\n",
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    './FashionMNIST',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor()]) # converting ToTensor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "286d4bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NetWork_5(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(NetWork_5, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "   \n",
    "        self.fc1 = nn.Linear(in_features=12*4*4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "        self.out = nn.Linear(in_features=60, out_features=10)\n",
    "        \n",
    "    def forward(self, t):\n",
    "        \n",
    "        t = F.relu(self.conv1(t))\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2) # will reduce the hieght and width our image\n",
    "        \n",
    "        t = F.relu(self.conv2(t))\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "        \n",
    "        t = F.relu(self.fc1(t.reshape(-1, 12*4*4)))\n",
    "        t = F.relu(self.fc2(t))\n",
    "        t = self.out(t)\n",
    "        \n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7a6416",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad6dbeec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "netWork = NetWork_5()\n",
    "\n",
    "sample_5 = next(iter(train_set)) ### will help you to get one data in a time\n",
    "images, labels = sample_5\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79311e24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753a4bf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8019def3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1, 5, 5])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netWork.conv1.weight.shape\n",
    "#### torch.Size([6, 1, 5, 5]) \n",
    "# 6 represents num of filters\n",
    "# 1 represents the depth of each filter\n",
    "# 5 and 5 represents the height and width of the filter\n",
    "# all filters represent using a single tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62ab44cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 28, 28])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3830d379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0567,  0.1057,  0.0664,  0.0659, -0.1346, -0.0653,  0.0039, -0.0737, -0.1034,  0.0175]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### prediction\n",
    "pred = netWork(images.unsqueeze(0))\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "192bfe2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68be4420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Compair with the real\n",
    "pred.argmax(dim=1).eq(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06263cca",
   "metadata": {},
   "source": [
    " # Make prediction for set or batch of data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45885927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 28, 28])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader = torch.utils.data.DataLoader(train_set, batch_size=10)\n",
    "batch_sample = next(iter(data_loader))\n",
    "images2, labels2 = batch_sample\n",
    "images2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94fd45ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0567,  0.1057,  0.0664,  0.0659, -0.1346, -0.0653,  0.0039, -0.0737, -0.1034,  0.0175],\n",
       "        [-0.0600,  0.1004,  0.0707,  0.0765, -0.1297, -0.0660,  0.0091, -0.0739, -0.1021,  0.0187],\n",
       "        [-0.0612,  0.1069,  0.0705,  0.0777, -0.1232, -0.0628, -0.0021, -0.0636, -0.1026,  0.0247],\n",
       "        [-0.0595,  0.1072,  0.0716,  0.0764, -0.1264, -0.0664,  0.0017, -0.0675, -0.1046,  0.0211],\n",
       "        [-0.0603,  0.1025,  0.0716,  0.0734, -0.1275, -0.0665,  0.0069, -0.0716, -0.0978,  0.0190],\n",
       "        [-0.0622,  0.1035,  0.0705,  0.0727, -0.1351, -0.0626,  0.0033, -0.0744, -0.0984,  0.0194],\n",
       "        [-0.0579,  0.1092,  0.0744,  0.0750, -0.1325, -0.0598,  0.0026, -0.0747, -0.0985,  0.0171],\n",
       "        [-0.0667,  0.1035,  0.0658,  0.0692, -0.1296, -0.0653,  0.0043, -0.0673, -0.0977,  0.0229],\n",
       "        [-0.0669,  0.1136,  0.0660,  0.0728, -0.1313, -0.0626, -0.0009, -0.0673, -0.1090,  0.0214],\n",
       "        [-0.0607,  0.1072,  0.0679,  0.0728, -0.1357, -0.0659,  0.0018, -0.0739, -0.1002,  0.0184]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred2 = netWork(images2)\n",
    "pred2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978670da",
   "metadata": {},
   "source": [
    "# Compaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "186bff37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False, False, False, False, False, False])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred2.argmax(dim=1).eq(labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0869139",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974953ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e7ba66a",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf08ce98",
   "metadata": {},
   "source": [
    "##### To find the best fit line U need to calculate the difference b/n \n",
    "***the true(the true dots) value and estimate-dots(predicted-value. distance from the line to the axis) ***\n",
    "##### use loss function for that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da15357",
   "metadata": {},
   "source": [
    "# Y^ -> y-hot means the points of the best fit line -- formula is y = mx+c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "48c9828b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d606c2",
   "metadata": {},
   "source": [
    "# Single linear regression\n",
    "##### Slope means what is the change value of y if the x value changes by any num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e7765a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the value of m and c  based the formula y=mx+c\n",
    "\n",
    "# y = mx + c\n",
    "\n",
    "# y = y-value\n",
    "# x = x-value\n",
    "# m = slope\n",
    "# c = intercept\n",
    "\n",
    "# let's take two data-point\n",
    "p1 = (2, 7)\n",
    "p2 = (3, 9)\n",
    "\n",
    "# first find the slope\n",
    "# m = dy/dx we can use this as well d-represents differecnc\n",
    "\n",
    "m = (y2-y1 / x2-x1)\n",
    "m = 9-7/3-2\n",
    "m = 2  \n",
    "\n",
    "# now find c by taking another datapoint p1 = (3, 9)\n",
    "# c = y - mx\n",
    "# c = 9 - 2(3)\n",
    "# c = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f3b16723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear equation  Y = mx + c\n",
    "\n",
    "# Single Linear Regression   Y' = B0 + BX\n",
    "\n",
    "# Multiple  Linear Regression Y' = B0 + B1X1 + B2X2 + B3X1X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "96e5a1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "array_2d = np.zeros((2, 3))\n",
    "print(array_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c21ca9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# for 1-DIMENSIONAL ARRAYS the np.dot computes the vector(array) dot product\n",
    "# So The output is a single scalar value\n",
    "\n",
    "c = np.array([3, 4, 5])\n",
    "q = [7, 8, 9]\n",
    "c.dot(q)\n",
    "\n",
    "# or this way\n",
    "a = [3, 4, 5]     \n",
    "b = [7, 8, 9]\n",
    "np.dot(a, b)      # ------> 3*7 + 4*8 + 5*9 = 98\n",
    "output = 98\n",
    "\n",
    "\n",
    "# IF ONE OF THE INPUTS IS A SCALAR, NP.DOT PERFORMS SCALAR MULTIPLICATION\n",
    "r = 2\n",
    "k = [7, 8, 9]\n",
    "np.dot(r,k)  \n",
    "output = [14, 16, 18]\n",
    "\n",
    "\n",
    "# Let’s say that you have two 2D arrays,.\n",
    "v = [[3, 4, 5], [6, 7, 8]]\n",
    "z = [[10, 11], [12, 13], [14, 15]]\n",
    "np.dot(v,z)\n",
    "output = [ [148, 160], [256, 277] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3baba03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic_Regression:\n",
    "    \n",
    "    def __init__(self, lr=0.0001, no_itr=1000):\n",
    "        self.lr = lr\n",
    "        self.no_itr = no_itr\n",
    "        self.weighst = None\n",
    "        self.bias = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        \n",
    "        # ** initiate the weight and base **\n",
    "        # y = wX + b\n",
    "        # w = w - α*dw  -> we are useing - b/c we are going down and α(alpha) shows the learning rate (how far we go down)\n",
    "        # b = b - α*db\n",
    "        # db = formula\n",
    "        # dw = formula\n",
    "        \n",
    "        \n",
    "        # gradient descent\n",
    "        for i in range(self.no_itr):\n",
    "            #y           =        wX               +      b    \n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            y_predi = self.sigmoidFunc(linear_model) # converts the -ve to +ve and create the S-curved line(b/n 0-1)\n",
    "            \n",
    "            # update the w and b  ======> w = w - α*dw and  # b = b - α*db\n",
    "            \n",
    "            dw = (1/n_samples) * np.dot(X.T, (y_predi - y))\n",
    "            db = (1/n_samples) * np.sum(y_predi - y)\n",
    "            \n",
    "            self.weights = self.weights - self.lr*dw\n",
    "            self.bias = self.bias - self.lr*db\n",
    "            \n",
    "                \n",
    "    def predict(self, x):\n",
    "        #y = wX + b\n",
    "        linear_model = np.dot(x, self.weights) + self.bias\n",
    "        y_predi = self.sigmoidFunc(linear_model)\n",
    "        predicted_y_cls = [1 if i > 0.5 else 0 for i in y_predi]\n",
    "        return predicted_y_cls\n",
    "    \n",
    "    def accuracy(self, real_ytest, predi_y):\n",
    "        #_y = real_ytest.reshape(-1, 1)\n",
    "        # the booli will return [[True, False, False, True, True, False]] then it will add only the True values\n",
    "        # np.sum([[True, False, False, True, True, False]])\n",
    "        booli = (predi_y == real_ytest)\n",
    "        acc = np.sum(booli)/len(real_ytest)\n",
    "        return acc\n",
    "\n",
    "\n",
    "\n",
    "    def sigmoidFunc(self, x):\n",
    "        return (1/1+np.exp(-x))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21fb4c8",
   "metadata": {},
   "source": [
    "# This simple and straightforward equation works for linear regression because linear regression uses a simple linear equation: (Y= A+BX).\n",
    "# But logistic regression uses a sigmoid function which is not linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ba40f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b820959",
   "metadata": {},
   "source": [
    "# Test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "08159d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-61-6be4b7015f2e>:68: RuntimeWarning: overflow encountered in exp\n",
      "  return (1/1+np.exp(-x))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.38596491228070173"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we can test it by feeding dataSet\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "breast_cancer = load_breast_cancer()\n",
    "\n",
    "# split\n",
    "X = breast_cancer['data']\n",
    "y = breast_cancer['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1234)\n",
    "\n",
    "\n",
    "# train the model\n",
    "my_model = Logistic_Regression()\n",
    "my_model.fit(X_train, y_train)\n",
    "predi_result = my_model.predict(X_test)\n",
    "print(predi_result)\n",
    "\n",
    "\n",
    "# accu\n",
    "my_model.accuracy(y_test, predi_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207af893",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94db555b",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.sum((np.array([8,4]) - np.array([6, 6]))) \n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d06484",
   "metadata": {},
   "outputs": [],
   "source": [
    "dw = (1/m)*(y_hat — y).X\n",
    "db = (1/m)*(y_hat — y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6645e338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "569 30\n",
      "569\n",
      "569\n"
     ]
    }
   ],
   "source": [
    "def gradients(X, y, y_hat):\n",
    "    \n",
    "    # X --> Input.\n",
    "    # y --> true/target value.\n",
    "    # y_hat --> hypothesis/predictions.\n",
    "    # w --> weights (parameter).\n",
    "    # b --> bias (parameter).\n",
    "    \n",
    "    # m-> number of training examples.\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # Gradient of loss w.r.t weights.\n",
    "    dw = (1/m)*np.dot(X.T, (y_hat - y))\n",
    "    \n",
    "    # Gradient of loss w.r.t bias.\n",
    "    db = (1/m)*np.sum((y_hat - y)) \n",
    "    \n",
    "    return dw, db\n",
    "\n",
    "# gradients(X, y, np.array([6, 8]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18728a20",
   "metadata": {},
   "source": [
    "# Resahpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a0a0c712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "569 30\n",
      "569\n",
      "569\n"
     ]
    }
   ],
   "source": [
    "def reshape(X, y):\n",
    "    # n_fe-> number of training examples\n",
    "    # n_sa-> number of features \n",
    "    n_fe, n_sa = X.shape\n",
    "    print(n_fe, n_sa)\n",
    "    print(len(y))\n",
    "    #print(y.reshape(n_fe, 1))\n",
    "    print(len(y.reshape(n_fe, 1))) # meaning we're converting it from [0,1,1,0,0] to [[0],[1],[1],[0],[0]]\n",
    "    \n",
    "reshape(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b2f4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, we want to know how our hypothesis(y_hat) is going to make predictions of whether y=1 or y=0.\n",
    "y=1 when y_hat ≥ 0.5\n",
    "y=0 when y_hat < 0.5\n",
    "\n",
    "# meaning\n",
    "y=1 when w.X + b ≥ 0\n",
    "y=0 when w.X + b < 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "14b062ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2],\n",
       "       [3, 4, 5],\n",
       "       [6, 7, 8]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshape\n",
    "b = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8])\n",
    "b.reshape(3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122be33d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
